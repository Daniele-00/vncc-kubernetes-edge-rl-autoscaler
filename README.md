# Kubernetes Edge Cloud with Reinforcement Learning Autoscaler

Progetto per lâ€™insegnamento **Virtual Networks and Cloud Computing** (a.a. **2024/25**)  
Autore: **<metti-il-tuo-nome>**

Questo progetto implementa un mini **Edge Cloud** basato su **Kubernetes** in cui il numero di repliche di un microservizio viene gestito da un autoscaler intelligente basato su **Reinforcement Learning (Q-learning)**.

Lâ€™obiettivo Ã¨ mostrare, in modo riproducibile, come sia possibile:

- containerizzare unâ€™applicazione â€œedgeâ€
- orchestrarla con Kubernetes
- generare un carico variabile
- misurare la latenza
- usare un agente RL per decidere dinamicamente il numero di repliche (scaling up/down) in funzione delle prestazioni

Il repository include:

- codice dellâ€™applicazione
- manifest Kubernetes
- autoscaler RL e baseline
- script di generazione carico
- strumenti per visualizzare i risultati (**Plotly + Streamlit**)
- istruzioni complete per riprodurre lâ€™esperimento

---

## ğŸ” Overview dellâ€™architettura

### Architettura logica (high level)

```text
                +----------------------------------------+
                |              Load Generator             |
                |  (Python requests: traffico variabile)  |
                +-------------------------+--------------+
                                          |  HTTP
                                          v
                               +------------------------+
                               |   Kubernetes (minikube)|
                               | - Deployment: edge-app |
           NodePort:30080  +-->| - Service: NodePort    |
                           |   | - Pod(s): Docker       |
                           |   +------------------------+
                           |              ^
                           |              | HTTP
                           |              |
                           |   +-----------------------+
                           |   |     Edge App (Flask)  |
                           |   |   CPU-bound workload  |
                           |   +-----------------------+
                           |
                           |  Log CSV (lat,replicas,reward)
                           v
                 +--------------------------+
                 |  RL Autoscaler (Q-Learn) |
                 | - misura latenza         |
                 | - decide scaling         |
                 +--------------------------+
                             |
                             |  kubectl scale
                             v
                         Kubernetes

                 +--------------------------+
                 | Dashboard (Streamlit)    |
                 | + Plotly (grafici live)  |
                 +--------------------------+
```

---

## ğŸ“ Struttura del repository

```text
kube-rl-edge/
â”‚
â”œâ”€â”€ app/                     # Applicazione edge (Flask) + Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ k8s/                     # Manifest Kubernetes
â”‚   â””â”€â”€ deployment.yaml      # Deployment + Service NodePort
â”‚
â”œâ”€â”€ autoscaler/              # Autoscaler intelligenti
â”‚   â”œâ”€â”€ rl_autoscaler.py     # Autoscaling RL (Q-learning)
â”‚   â””â”€â”€ baseline_autoscaler.py   # Autoscaling a soglia fissa (baseline)
â”‚
â”œâ”€â”€ load/                    # Generatore di traffico
â”‚   â””â”€â”€ load_generator.py
â”‚
â”œâ”€â”€ results/                 # Log CSV e grafici generati
â”‚   â”œâ”€â”€ rl_log.csv           # Log autoscaler RL
â”‚   â””â”€â”€ baseline_log.csv     # Log baseline (se eseguita)
â”‚
â”œâ”€â”€ dashboard_pretty.py      # Dashboard Streamlit â€œsempliceâ€
â”œâ”€â”€ dashboard_ultra.py       # Dashboard Streamlit avanzata (Plotly, soglie, colori)
â”œâ”€â”€ plot_results.py          # Analisi e grafici offline (Plotly)
â”œâ”€â”€ requirements.txt         # Dipendenze Python (consigliato)
â””â”€â”€ README.md
```

> Se `requirements.txt` non esiste ancora, puoi crearlo con le librerie usate: `flask`, `requests`, `numpy`, `pandas`, `plotly`, `streamlit`.

---

## ğŸ§± Prerequisiti

### Opzione A â€“ Ambiente tipico (Windows + WSL2)

- Windows 10/11
- WSL2 con Ubuntu 22.04 (o simile)
- Supporto virtualizzazione attivo nel BIOS

Dentro Ubuntu (WSL2):

- Docker Engine
- minikube
- kubectl
- Python 3.10+ e `python3-venv`

### Opzione B â€“ Linux nativo

- Distribuzione Linux (Ubuntu consigliato)
- Docker Engine
- minikube
- kubectl
- Python 3.10+ e `python3-venv`

---

## âš™ï¸ Setup passoâ€“passo

### 1ï¸âƒ£ Clonare il repository

```bash
git clone https://github.com/<tu-utente>/<nome-repo>.git
cd <nome-repo>
```

### 2ï¸âƒ£ Creare e attivare un virtual environment Python

```bash
python3 -m venv venv
source venv/bin/activate
```

Installare le dipendenze:

```bash
pip install --upgrade pip
pip install flask requests numpy pandas plotly streamlit
# oppure, se presente:
# pip install -r requirements.txt
```

### 3ï¸âƒ£ Avviare minikube

```bash
minikube start --driver=docker
kubectl get nodes
```

### 4ï¸âƒ£ Build dellâ€™immagine Docker dellâ€™applicazione edge

```bash
cd app
docker build -t edge-app:latest .
cd ..
```

Caricare lâ€™immagine dentro minikube:

```bash
minikube image load edge-app:latest
```

### 5ï¸âƒ£ Deploy su Kubernetes

```bash
kubectl apply -f k8s/deployment.yaml
kubectl get pods
kubectl get svc
```

Recuperare lâ€™IP del nodo minikube:

```bash
minikube ip
# es. 192.168.49.2
```

Test (da Ubuntu/WSL):

```bash
curl http://<MINIKUBE_IP>:30080/
# output atteso: OK
```

### 6ï¸âƒ£ Configurare gli script con lâ€™IP di minikube

Nei file:

- `load/load_generator.py`
- `autoscaler/rl_autoscaler.py`
- `autoscaler/baseline_autoscaler.py` (se usata)

sostituire:

```python
MINIKUBE_IP = "<MINIKUBE_IP>"
```

con lâ€™IP reale, ad esempio:

```python
MINIKUBE_IP = "192.168.49.2"
URL = f"http://{MINIKUBE_IP}:30080/"
```

---

## ğŸš€ Esecuzione della demo completa

Usa **4 terminali** (tutti con `source venv/bin/activate`).

### Terminale 1 â€“ Monitor Kubernetes

Mostra in tempo reale il numero di repliche:

```bash
kubectl get deploy edge-app -w
```

### Terminale 2 â€“ Generatore di carico

```bash
python load/load_generator.py
```

Output tipico:

```text
Calma: 0.06...
Carico: 0.20...
Calma: 0.07...
Carico: 0.19...
```

### Terminale 3 â€“ Autoscaler RL (Q-learning)

```bash
python autoscaler/rl_autoscaler.py
```

Output esemplificativo:

```text
Episode 0: lat=0.210s, replicas=1, reward=0.00
deployment.apps/edge-app scaled
Episode 1: lat=0.120s, replicas=2, reward=3.00
Episode 2: lat=0.085s, replicas=2, reward=4.00
...
```

Genera il file:

- `results/rl_log.csv`

In parallelo, nel Terminale 1 compaiono nuove righe quando cambia il numero di repliche:

```text
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
edge-app    1/1     1            1           5m
edge-app    2/2     2            2           6m   # scaling up
...
```

### Terminale 4 â€“ Dashboard interattiva

Dashboard avanzata:

```bash
streamlit run dashboard_ultra.py --server.address=localhost --server.port=8502
```

Nel browser:

- `http://localhost:8502`

La dashboard mostra:

- KPI: latenza media, repliche medie, reward medio
- grafico combinato latenza + repliche (con soglie)
- grafico reward nel tempo
- tabella delle ultime decisioni dellâ€™agente

---

## ğŸ“Š Analisi offline dei risultati

Dopo una run dellâ€™RL (e opzionalmente della baseline):

```bash
python plot_results.py
```

Lo script legge:

- `results/rl_log.csv`
- `results/baseline_log.csv` (se presente)

e genera grafici HTML, ad es.:

- `results/rl_latency.html`
- `results/rl_replicas.html`
- `results/rl_reward.html`

Apribili da browser (doppio click su Windows / Linux).

---

## ğŸ§  Reinforcement Learning in breve

Lâ€™autoscaler RL modella il problema come un **Markov Decision Process**:

- **Stato** \(s\):
  - livello di latenza (bassa / media / alta)
  - numero corrente di repliche
- **Azioni** \(a\):
  - \(a \in \{-1, 0, +1\}\) (diminuire, mantenere, aumentare repliche)
- **Reward**:
  - positivo se la latenza Ã¨ sotto soglia con poche repliche
  - negativo se la latenza Ã¨ alta o vengono usate troppe repliche

Aggiornamento Q-learning:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
\]

Strategia di scelta azioni: **Îµ-greedy** (equilibrio tra esplorazione e sfruttamento).

---

## ğŸ”¬ Baseline autoscaler

La baseline implementa una politica piÃ¹ semplice:

- se la latenza > soglia alta â†’ scala â€œupâ€ (aumenta repliche)
- se la latenza < soglia bassa â†’ scala â€œdownâ€ (riduce repliche)
- altrimenti mantiene

I log vengono salvati in `results/baseline_log.csv` e possono essere confrontati con lâ€™RL tramite `plot_results.py`.

---

## âœ… Possibili estensioni

- Epsilon decay (ridurre esplorazione nel tempo)
- PenalitÃ  per troppi cambi di repliche (stabilitÃ )
- Stati arricchiti (CPU, throughput, percentili di latenza)
- RL con approssimazione di funzione (DQN)
- Edge multi-nodo / multi-servizio

---

## ğŸ’¡ Note per il docente

Il progetto Ã¨ pensato per essere:

- **didattico**: mostra chiaramente Docker, Kubernetes, autoscaling, RL
- **riproducibile**: tutti i comandi necessari sono elencati
- **estensibile**: la struttura a directory separa chiaramente app, manifest K8s, RL, carico, analisi

In caso di problemi di compatibilitÃ  con WSL2, il progetto Ã¨ facilmente eseguibile anche su una macchina Linux nativa con Docker e minikube giÃ  installati.
