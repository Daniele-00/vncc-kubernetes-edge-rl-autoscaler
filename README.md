# <img src="logo/logo_vncc.png" alt="Logo" width="40" style="vertical-align: middle;"> Kubernetes RL Autoscaler for Edge Computing

[![Python](https://img.shields.io/badge/Python-3.12%2B-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.34-326CE5?logo=kubernetes&logoColor=white)](https://kubernetes.io/)
[![Docker](https://img.shields.io/badge/Docker-28.4-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-v1.51-FF4B4B?logo=streamlit&logoColor=white)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Corso:** Virtual Networks and Cloud Computing (A.A. 2024/2025)  
> **Autore:** Daniele Nanni Cirulli (368204)  
> **UniversitÃ  degli Studi di Perugia** - Dipartimento di Ingegneria

---

## Indice

- [Panoramica del Progetto](#panoramica-del-progetto)
- [Problema e Soluzione](#problema-e-soluzione)
- [Caratteristiche Principali](#caratteristiche-principali)
- [Architettura del Sistema](#ï¸architettura-del-sistema)
- [Tecnologie Utilizzate](#tecnologie-utilizzate)
- [Quick Start](#quick-start)
- [Installazione Dettagliata](#installazione-dettagliata)
- [Metodologia Training/Evaluation](#metodologia-trainingevaluation)
- [Modello Teorico (Q-Learning)](#modello-teorico-q-learning)
- [Risultati Sperimentali](#risultati-sperimentali)
- [Struttura del Progetto](#struttura-del-progetto)
- [Contatti](#contatti)

---

## Panoramica del Progetto

Questo progetto implementa un sistema di **autoscaling intelligente** per ambienti **Edge Computing** utilizzando **Reinforcement Learning (Q-Learning)**. Il sistema Ã¨ stato sviluppato come applicazione cloud-native completamente funzionante su **Kubernetes (Minikube)**, dimostrando come l'apprendimento automatico possa superare approcci tradizionali rule-based nella gestione dinamica delle risorse.

### Contributi Metodologici Chiave

- **Separazione Training/Evaluation**: Fase di addestramento (34 min, 200 episodi) separata da fase di valutazione per confronto scientifico rigoroso
- **Reward Zone-Based**: Funzione reward adattiva con pesi dinamici che variano in base allo stato del sistema
- **Training Curriculum**: 6 super-cicli randomizzati (calma/onda/spike) per evitare overfitting
- **Metrica SLA Met**: Percentuale di episodi con latenza ottimale, piÃ¹ informativa delle semplici violazioni

### Risultati Principali

| Metrica | Baseline | RL (Eval) | Delta |
|---------|----------|-----------|-------|
| **SLA Met** | 86.6% | **95.1%** | **+8.5 punti** âœ… |
| Latenza Media | 0.290s | 0.264s | -8.9% âœ… |
| Repliche Medie | 2.34 | 2.56 | +9.4% âš ï¸ |
| Efficiency (SLA/Cost) | 0.370 | 0.371 | Pari âœ… |

---

## Problema e Soluzione

### Il Problema: Autoscaling in Edge Computing

L'Edge Computing sposta l'elaborazione dai datacenter centralizzati verso dispositivi periferici, introducendo sfide uniche:

- **Risorse Limitate**: CPU, memoria ed energia ridotte sui nodi Edge
- **Carichi Imprevedibili**: Traffico altamente variabile e difficile da predire
- **Latenza Critica**: NecessitÃ  di tempi di risposta bassi per applicazioni real-time
- **Efficienza Economica**: Minimizzare costi evitando over-provisioning

### Limiti degli Approcci Tradizionali

Gli autoscaler rule-based (es. Kubernetes HPA) utilizzano soglie fisse:

```python
if latency > HIGH_THRESHOLD:
    scale_up()
elif latency < LOW_THRESHOLD:
    scale_down()
```

**Problemi:**
- Reazione ritardata (attendono superamento soglie)
- Assenza di memoria storica
- Configurazione manuale per ogni workload
- Comportamento subottimale sotto carichi variabili

### La Soluzione: Q-Learning con Train/Eval Split

L'agente RL apprende una policy ottimale in due fasi distinte:

1. **Training (34 min)**: Esplora lo spazio stati-azioni su pattern randomizzati, costruisce Q-Table
2. **Evaluation**: Usa Q-Table congelata (zero esplorazione) per confronto equo con baseline

**Vantaggi:**
- Apprende strategie adattive senza regole esplicite
- Bilancia automaticamente latenza e costi tramite reward
- Policy piÃ¹ reattiva (adatta repliche seguendo carico istantaneo)
- Confronto scientifico valido (policy entrambe deterministiche)

---

## Caratteristiche Principali

### Core Features

- **Agente Q-Learning Tabular**
  - 15 stati discreti: {3 bucket latenza} Ã— {5 livelli repliche}
  - 3 azioni: Scale Down (-1), Hold (0), Scale Up (+1)
  - Epsilon-greedy decay: da 0.9 a 0.1 in ~150 episodi
  - Q-Table salvata/caricata per separazione train/eval

- **Reward Function Zone-Based Adattiva**
  - **SLA Term**: Interpolazione lineare in zona TARGET (da +2 a +6)
  - **Cost Term**: Peso dinamico 0.5 (HIGH) / 1.0 (TARGET) / 2.0 (LOW)
  - **Shape Term**: Bonus +2 per UP in HIGH, +1 per DOWN in LOW
  - Risolve "pigrizia" nello scale-up e spreco in calma

- **Training Curriculum Randomizzato**
  - 6 super-cicli con scenari calma/onda/spike
  - Ordine e durata randomizzati (seed fisso per riproducibilitÃ )
  - Evita overfitting su sequenze specifiche
  - Genera ~200 episodi in 34 minuti

- **Baseline Rule-Based per Confronto**
  - Logica threshold pura (IF latency > soglia THEN scale)
  - Reward post-hoc con stessa funzione dell'RL (confronto equo)
  - Evidenzia limiti approcci reattivi senza memoria

- **Dashboard Streamlit Avanzata**
  - 3 modalitÃ : RL / Baseline / Confronto Diretto
  - **Zone Semantiche**: Grafici con bande colorate (verde=calma, giallo=onda, arancione=spike)
  - Modifiche SLA hot-reload (no restart autoscaler)
  - Box plot distribuzione + tabelle comparative

- **Load Generator Multi-Thread**
  - 15 worker concorrenti per saturazione realistica
  - Calibrato per creare contention (200ms delay Ã— 15 thread = latenze osservabili)
  - 4 scenari: Calma (2 req/s/thread), Spike (20 req/s), Onda (sinusoidale), Stop

---

## Architettura del Sistema

<p align="center">
  <img src="logo/architettura.png" alt="Architettura del Sistema" width="100%">
</p>

### Flusso Dettagliato

1. **Monitor**: Autoscaler misura latenza media (batch di richieste di prova)
2. **Analyze**: Discretizza latenza in bucket (LOW/TARGET/HIGH)
3. **Plan**: Seleziona azione
   - RL: Consulta Q-Table appresa â†’ `a = argmax Q(s, a)`
   - Baseline: Applica regola IF-THEN
4. **Execute**: `kubectl scale deploy --replicas=N`
5. **Log**: Persist metriche su CSV per dashboard e analisi offline

---

## Tecnologie Utilizzate

| Componente | Versione | Ruolo |
|-----------|---------|-------|
| **Python** | 3.12.3 | Linguaggio principale (RL, autoscaler, load) |
| **Kubernetes** | v1.34.0 | Orchestrazione container |
| **Minikube** | v1.37.0 | Cluster locale single-node |
| **Docker** | 28.4.0 | Containerizzazione microservizi |
| **Flask** | 3.1.2 | Microservizio Edge app |
| **Streamlit** | 1.51.0 | Dashboard web interattiva |
| **Plotly** | 6.4.0 | Grafici interattivi (time series, box plot) |
| **NumPy** | 2.3.4 | Operazioni Q-Table |
| **Pandas** | 2.3.3 | Manipolazione CSV logs |

**Ambiente**: Windows 11 + WSL 2 Ubuntu 24.04

---

## Quick Start

```bash
# 1. Setup cluster
minikube start --driver=docker --cpus=4 --memory=4096
eval $(minikube docker-env)

# 2. Build & Deploy
docker build -t edge-app:latest ./app
kubectl apply -f k8s/deployment.yaml

# 3. Terminale 1: Monitor Kubernetes
kubectl get deploy edge-app -w

# 4. Terminale 2: Load Generator
export MINIKUBE_IP=$(minikube ip)
python load/load_controller.py

# 5. Terminale 3: RL Autoscaler (EVAL mode)
export RL_MODE=eval  # Usa Q-Table giÃ  addestrata
python autoscaler/rl_autoscaler.py

# 6. Terminale 4: Dashboard
streamlit run ui/dashboard.py
# Vai su http://localhost:8501
```

---

## Installazione Dettagliata

<details>
<summary>â• Clicca per espandere</summary>

### Prerequisiti

#### Windows
```powershell
# Installa Docker Desktop, Minikube, kubectl
# Link: https://docs.docker.com/desktop/install/windows-install/
```

#### macOS
```bash
brew install minikube kubectl docker
```

#### Linux (Ubuntu/Debian)
```bash
# Docker
curl -fsSL https://get.docker.com | sh

# Minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# kubectl
curl -LO "https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install kubectl /usr/local/bin/kubectl
```

### Setup Progetto

```bash
# Clone repository
git clone https://github.com/Daniele-00/vncc-kubernetes-edge-rl-autoscaler.git
cd vncc-kubernetes-edge-rl-autoscaler

# Virtual environment Python
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Test connettivitÃ 
export MINIKUBE_IP=$(minikube ip)
curl http://$MINIKUBE_IP:30080  # Output atteso: OK (dopo ~200ms)
```

</details>

---

## Metodologia Training/Evaluation

### PerchÃ© Separare Training ed Evaluation?

Senza separazione, il confronto RL vs Baseline sarebbe ingiusto: l'RL includerebbe episodi di esplorazione casuale (azioni subottimali intenzionali) che penalizzerebbero artificialmente le sue performance.

### Protocollo Implementato

#### Fase 1: Training (RL\_MODE=train)

```bash
export RL_MODE=train
python autoscaler/rl_autoscaler.py
python benchmark/training_benchmark.py  # Orchestr curriculum
```

- **Durata**: ~34 minuti (~200 episodi)
- **Scenari**: 6 super-cicli randomizzati (calma/onda/spike)
- **Epsilon**: Decay da 0.9 â†’ 0.1
- **Output**: Q-Table salvata su `results/qtable.npy`
- **Log**: `results/rl_train_log.csv` (include epsilon per analisi convergenza)

#### Fase 2: Evaluation RL (RL\_MODE=eval)

```bash
kubectl scale deploy edge-app --replicas=1  # Reset
export RL_MODE=eval
python autoscaler/rl_autoscaler.py
python benchmark/benchmark.py  # Scenario DETERMINISTICO
```

- **Q-Table**: Caricata da disco (frozen)
- **Epsilon**: Forzato a 0 (zero esplorazione)
- **Scenario**: Onda deterministico (identico per baseline)
- **Output**: `results/rl_eval_log.csv`

#### Fase 3: Evaluation Baseline

```bash
kubectl scale deploy edge-app --replicas=1  # Reset
python autoscaler/baseline_autoscaler.py
python benchmark/benchmark.py  # STESSO scenario RL
```

- **Output**: `results/baseline_log.csv`

### Differenza Chiave: training\_benchmark.py vs benchmark.py

| Script | Fase | Randomizzazione | Scopo |
|--------|------|-----------------|-------|
| `training_benchmark.py` | Train | Ordine + durata casuale | Irrobustire policy, evitare overfitting |
| `benchmark.py` | Eval | Sequenza fissa deterministica | Confronto equo RL vs Baseline |

---

## Modello Teorico (Q-Learning)

### MDP Formulation

**Stato**: $s = (l, n)$ dove $l \in \{0, 1, 2\}$ (bucket latenza), $n \in \{1,...,5\}$ (repliche)  
â†’ **15 stati totali**

**Azioni**: $\mathcal{A} = \{-1, 0, +1\}$ (Scale Down, Hold, Scale Up)

**Reward Zone-Based**:

$$
R(s, a) = \underbrace{R_{\text{SLA}}(l)}_{\text{QoS}} + \underbrace{R_{\text{cost}}(n, l)}_{\text{Efficienza}} + \underbrace{R_{\text{shape}}(a, l)}_{\text{Bias}}
$$

**Q-Learning Update**:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
$$

Parametri: $\alpha=0.1$, $\gamma=0.9$, $\epsilon(t) = \max(0.9 \cdot 0.985^t, 0.1)$

---

## Risultati Sperimentali

### Setup

- **Hardware**: Intel i7 (8 core), 16 GB RAM, WSL 2 Ubuntu
- **Scenario Eval**: Onda deterministica (identico per RL e Baseline)
- **Soglie SLA**: Low=0.25s, High=0.35s
- **Episodi**: RL 205, Baseline 82 (test a regime)

### Metriche Comparative

| Metrica | Baseline | RL (Eval) | Interpretazione |
|---------|----------|-----------|-----------------|
| **Latenza Media** | 0.290s | 0.264s | RL 8.9% piÃ¹ veloce âœ… |
| **Repliche Medie** | 2.34 | 2.56 | RL 9.4% piÃ¹ costoso âš ï¸ |
| **SLA Met** | **86.6%** | **95.1%** | RL +8.5 punti âœ… |
| **Violazioni SLA** | 11 | 10 | 1 violazione evitata âœ… |
| **Efficiency** | 0.370 | 0.371 | Sostanzialmente pari âœ… |

### Interpretazione Chiave

**QualitÃ  del Servizio**: L'RL mantiene latenza ottimale nel 95.1% degli episodi contro 86.6% della baseline. Questo si traduce in un'esperienza utente significativamente migliore in applicazioni Edge latency-sensitive.

**Trade-off Costi/QoS**: L'RL utilizza 9.4% di risorse in piÃ¹, ma l'**SLA Efficiency** (0.371 vs 0.370) Ã¨ identica. Questo indica che il sistema ha raggiunto un equilibrio ottimale: l'overhead Ã¨ proporzionale al miglioramento qualitativo.

**ReattivitÃ  vs StabilitÃ **: L'RL adatta piÃ¹ frequentemente le repliche seguendo il carico istantaneo (comportamento dinamico), mentre la baseline sembra piÃ¹ stabile ma Ã¨ solo piÃ¹ lenta a reagire. Questa maggiore reattivitÃ  dell'RL Ã¨ il **prezzo** da pagare per ottenere 95.1% SLA Met.

### Grafici

![Confronto Serie Temporali](results/confronto_eval.png)

*Le bande colorate verticali indicano lo scenario attivo: verde (calma), giallo (onda), arancione (spike). L'RL adatta dinamicamente le repliche, la baseline mantiene configurazioni piÃ¹ stabili ma meno reattive.*

---

## Struttura del Progetto

```
kube-rl-edge/
â”œâ”€â”€ app/                          # Microservizio Edge
â”‚   â”œâ”€â”€ app.py                    # Flask server (200ms delay)
â”‚   â””â”€â”€ Dockerfile                # Containerizzazione
â”œâ”€â”€ autoscaler/                   # Logica Autoscaling
â”‚   â”œâ”€â”€ rl_autoscaler.py          # Agente Q-Learning (train/eval)
â”‚   â”œâ”€â”€ baseline_autoscaler.py    # Controller rule-based
â”‚   â””â”€â”€ reward_utils.py           # Reward function
â”œâ”€â”€ k8s/                          # Manifest Kubernetes
â”‚   â””â”€â”€ deployment.yaml           # Deployment + Service NodePort
â”œâ”€â”€ benchmark/                    # Scripts Benchmark
â”‚   â”œâ”€â”€ training_benchmark.py     # Curriculum randomizzato (train)
â”‚   â””â”€â”€ benchmark.py              # Scenario deterministico (eval)
â”œâ”€â”€ load/                         # Generazione Traffico
â”‚   â””â”€â”€ load_controller.py        # Multi-thread load (15 worker)
â”œâ”€â”€ ui/                           # Dashboard
â”‚   â””â”€â”€ dashboard.py              # Streamlit UI con zone semantiche
â”œâ”€â”€ plots/                        # Plotting
â”‚   â””â”€â”€ plot_compare.py           # Grafici confronto RL vs Baseline
â”œâ”€â”€ results/                      # Output
â”‚   â”œâ”€â”€ rl_train_log.csv          # Log training (con epsilon)
â”‚   â”œâ”€â”€ rl_eval_log.csv           # Log evaluation RL
â”‚   â”œâ”€â”€ baseline_log.csv          # Log evaluation baseline
â”‚   â””â”€â”€ qtable.npy                # Q-Table addestrata
â””â”€â”€ requirements.txt              # Dipendenze Python
```

---

## Sviluppi Futuri

- **Deep RL**: DQN per spazi stati continui e multi-dimensionali
- **Multi-Cluster**: Cooperative multi-agent RL per Edge distribuito
- **Transfer Learning**: Pre-training su workload sintetici + fine-tuning

---

## Contatti

**Daniele Nanni Cirulli**

- ğŸ“§ Email: [daniele.nannicirulli@studenti.unipg.it](mailto:daniele.nannicirulli@studenti.unipg.it)
- ğŸ”— LinkedIn: [linkedin.com/in/daniele-nanni-cirulli](https://www.linkedin.com/in/daniele-nanni-cirulli-6052b2231/)
- ğŸ™ GitHub: [@Daniele-00](https://github.com/Daniele-00)

**UniversitÃ  degli Studi di Perugia**  
Dipartimento di Ingegneria  
A.A. 2024/2025

---

<div align="center">

### â­ Se questo progetto ti Ã¨ stato utile, lascia una stella!

**Sviluppato con ğŸ’™ per il corso di Virtual Networks and Cloud Computing**

[ğŸ” Torna all'Inizio](#-kubernetes-rl-autoscaler-for-edge-computing)

</div>
